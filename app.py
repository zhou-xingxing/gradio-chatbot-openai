import os
import sys
import logging
from typing import Generator, TypedDict
from dotenv import load_dotenv
from openai import OpenAI, APIError, AuthenticationError, RateLimitError
import yaml

os.environ["GRADIO_ANALYTICS_ENABLED"] = "false"
import gradio as gr


# Configure logging
logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('chatbot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# Type definitions
class UserState(TypedDict):
    """User session state structure."""
    model_id: str
    context_size: int
    system_prompt: str
    enable_thinking: bool


# OpenAI client cache to avoid recreating clients
_client_cache: dict[str, OpenAI] = {}


def get_or_create_openai_client(model_id: str) -> OpenAI:
    """Create or get cached OpenAI client for the specified model."""
    if model_id not in _client_cache:
        model_config = get_model_config(model_id)
        _client_cache[model_id] = OpenAI(
            api_key=model_config['api_key'],
            base_url=model_config['base_url']
        )
    return _client_cache[model_id]


def load_config() -> dict:
    """Load configuration from config.yaml or environment variables."""
    config_path = os.path.join(os.path.dirname(__file__), 'config.yaml')

    if os.path.exists(config_path):
        # Use config.yaml only
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    else:
        # Fallback to environment variables
        config = {
            'models': [{
                'id': os.getenv('MODEL_ID', 'gpt-4o'),
                'api_key': os.getenv('API_KEY', ''),
                'base_url': os.getenv('BASE_URL', 'https://api.openai.com/v1'),
                'supports_thinking': False
            }]
        }

    # Set default values if not present in config
    if not config.get('context_size'):
        config['context_size'] = 5
    if not config.get('system_prompt'):
        config['system_prompt'] = 'You are a helpful AI assistant.'

    # Validate configuration
    validate_config(config)
    return config


def validate_config(config: dict) -> None:
    """Validate configuration and exit if invalid."""
    if not config.get('models'):
        logger.error("é…ç½®é”™è¯¯: models åˆ—è¡¨ä¸ºç©ºæˆ–æœªå®šä¹‰")
        sys.exit(1)

    for model in config['models']:
        if 'id' not in model:
            logger.error("é…ç½®é”™è¯¯: æ¨¡å‹ç¼ºå°‘å¿…éœ€çš„ 'id' å­—æ®µ")
            sys.exit(1)
        if 'api_key' not in model:
            logger.error(f"é…ç½®é”™è¯¯: æ¨¡å‹ '{model.get('id', 'unknown')}' ç¼ºå°‘ 'api_key'")
            sys.exit(1)
        if 'base_url' not in model:
            logger.error(f"é…ç½®é”™è¯¯: æ¨¡å‹ '{model.get('id', 'unknown')}' ç¼ºå°‘ 'base_url'")
            sys.exit(1)

    # Set default_model_id to first model if not specified
    model_ids = [m['id'] for m in config['models']]
    config.setdefault('default_model_id', model_ids[0])

# Load environment variables
load_dotenv()

# Load configuration
CONFIG = load_config()

# Build model config mapping and choices list
MODEL_CONFIG_MAP = {model['id']: model for model in CONFIG['models']}
MODEL_CHOICES = list(MODEL_CONFIG_MAP.keys())  # Dropdown choices are model IDs


def get_model_config(model_id: str) -> dict:
    """Get configuration for a specific model."""
    return MODEL_CONFIG_MAP.get(model_id, CONFIG['models'][0])


def create_user_state(enable_thinking: bool = True) -> UserState:
    """Create a new user-specific state."""
    return {
        "model_id": CONFIG['default_model_id'],
        "context_size": CONFIG['context_size'],
        "system_prompt": CONFIG['system_prompt'],
        "enable_thinking": enable_thinking
    }


def update_model(model_id: str, state: UserState) -> UserState:
    """Update the selected model."""
    state["model_id"] = model_id

    # Update thinking capability based on model
    model_config = get_model_config(model_id)
    if not model_config.get('supports_thinking', False):
        state["enable_thinking"] = False

    return state


def update_context_size(size: float | int, state: UserState) -> UserState:
    """Update the context size for conversation memory."""
    try:
        size = int(size)
        if size < 1:
            size = 1
        state["context_size"] = size
    except ValueError:
        pass
    return state


def update_system_prompt(prompt: str, state: UserState) -> UserState:
    """Update the system prompt."""
    if not prompt or prompt.strip() == "":
        prompt = CONFIG['system_prompt']
    state["system_prompt"] = prompt.strip()
    return state


def chat_response(message: str, history: list[dict] | None, state: UserState) -> Generator[str, None, None]:
    """Generate chat response using OpenAI API with streaming."""
    model_id = state.get("model_id", CONFIG['default_model_id'])
    model_config = get_model_config(model_id)

    # Build conversation history
    messages = []

    # Add system message
    messages.append({
        "role": "system",
        "content": state["system_prompt"]
    })

    # Add conversation history with context limit
    # Multiply by 2 because each round includes user message + assistant response
    recent_history = history[-state["context_size"]*2:] if history else []
    for msg in recent_history:
        if isinstance(msg, dict):
            role = msg.get("role", "")
            content = msg.get("content", "")

            # Handle Gradio 6.0 format: content is a list of dicts
            if isinstance(content, list):
                text_content = ""
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_content += item.get("text", "")
                content = text_content

            # Skip empty content
            if not content or not content.strip():
                continue

            # Extract content after thinking tags for assistant messages
            if role == "assistant":
                if ">> ## å®Œæ•´å›å¤" in content:
                    content = content.split(">> ## å®Œæ•´å›å¤")[-1].strip()
                elif "<details>" in content:
                    content = content.split("</details>")[-1].strip()

            messages.append({"role": role, "content": content})

    # Add current message
    messages.append({"role": "user", "content": message})

    try:
        # Create client for this model
        client = get_or_create_openai_client(model_id)

        # Build API request parameters
        api_params = {
            "model": model_id,
            "messages": messages,
            "stream": True,
        }

        # Only include thinking parameter if enabled and model supports it
        if state["enable_thinking"] and model_config.get('supports_thinking', False):
            api_params["extra_body"] = {
                "thinking": {
                    "type": "enabled",
                },
                "enable_thinking": True
            }

        # Call OpenAI API with streaming
        stream = client.chat.completions.create(**api_params)

        # Stream the response
        thinking_started = False
        thinking_ended = False

        for chunk in stream:
            delta = chunk.choices[0].delta

            # Check for reasoning content
            reasoning_content = getattr(delta, 'reasoning_content', None) or getattr(delta, 'reasoning', None)
            if reasoning_content and state["enable_thinking"] and model_config.get('supports_thinking', False):
                if not thinking_started:
                    thinking_started = True
                    yield ">> ## æ€è€ƒè¿‡ç¨‹\n\n"
                yield reasoning_content

            # Check for regular content
            if delta.content:
                content = delta.content
                if thinking_started and not thinking_ended and state["enable_thinking"] and model_config.get('supports_thinking', False):
                    thinking_ended = True
                    yield "\n\n>> ## å®Œæ•´å›å¤\n\n"
                yield content

    except AuthenticationError as e:
        logger.error(f"API è®¤è¯å¤±è´¥: {str(e)}")
        yield "é”™è¯¯: API å¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·æ£€æŸ¥é…ç½®"
    except RateLimitError as e:
        logger.error(f"API é™æµ: {str(e)}")
        yield "é”™è¯¯: API è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•"
    except APIError as e:
        logger.error(f"API é”™è¯¯: {str(e)}")
        yield f"é”™è¯¯: æ¨¡å‹æœåŠ¡å¼‚å¸¸ ({e.code if hasattr(e, 'code') else 'unknown'})"
    except Exception as e:
        logger.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
        yield f"é”™è¯¯: {str(e)}"


# Create Gradio interface
with gr.Blocks(title="AI Chatbot") as demo:
    gr.Markdown("# ğŸ¤– AI èŠå¤©æœºå™¨äºº")

    # Get default model's thinking support for initial state
    default_supports_thinking = MODEL_CONFIG_MAP[CONFIG['default_model_id']].get('supports_thinking', False)

    # User-specific state (isolated per session)
    user_state = gr.State(create_user_state(enable_thinking=default_supports_thinking))

    with gr.Row():
        with gr.Column(scale=3):
            # Chat interface
            chatbot = gr.Chatbot(
                label="å¯¹è¯",
                height=600
            )
            msg = gr.Textbox(
                label="è¾“å…¥æ¶ˆæ¯",
                placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                lines=2
            )
            with gr.Row():
                submit = gr.Button("å‘é€", variant="primary")
                clear = gr.Button("æ¸…é™¤å¯¹è¯")

        with gr.Column(scale=1):
            # Settings panel
            gr.Markdown("### âš™ï¸ è®¾ç½®")

            # Model selector
            model_dropdown = gr.Dropdown(
                label="é€‰æ‹©æ¨¡å‹",
                choices=MODEL_CHOICES,
                value=CONFIG['default_model_id'],
                info="é€‰æ‹©è¦ä½¿ç”¨çš„AIæ¨¡å‹"
            )

            # URL display (read-only, synced with model selection)
            model_url_display = gr.Textbox(
                label="API Base URL",
                value=MODEL_CONFIG_MAP[CONFIG['default_model_id']]['base_url'],
                interactive=False,
                info="æ¨¡å‹å¯¹åº”çš„APIåœ°å€"
            )

            # System prompt setting
            system_prompt = gr.Textbox(
                label="ç³»ç»Ÿæç¤ºè¯",
                value=CONFIG['system_prompt'],
                lines=3,
                placeholder="è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯...",
                info="å®šä¹‰æœºå™¨äººçš„è§’è‰²å’Œè¡Œä¸º"
            )
            update_prompt_btn = gr.Button("æ›´æ–°æç¤ºè¯", size="sm")

            context_size = gr.Number(
                label="å¯¹è¯è®°å¿†è½®æ•°",
                value=CONFIG['context_size'],
                minimum=1,
                maximum=50,
                step=1,
                info="æœºå™¨äººèƒ½è®°ä½çš„æœ€è¿‘å¯¹è¯è½®æ•°"
            )
            update_context_btn = gr.Button("æ›´æ–°è®°å¿†è®¾ç½®", size="sm")

            # Get default model's thinking support for initial state
            default_supports_thinking = MODEL_CONFIG_MAP[CONFIG['default_model_id']].get('supports_thinking', False)
            show_thinking = gr.Checkbox(
                label="å¯ç”¨æ€è€ƒèƒ½åŠ›",
                value=default_supports_thinking,
                interactive=default_supports_thinking,
                info="æ˜¯å¦å¯ç”¨AIçš„æ€è€ƒåŠŸèƒ½ï¼ˆä»…éƒ¨åˆ†æ¨¡å‹æ”¯æŒï¼‰"
            )

    # Event handlers
    def submit_message(message: str, history: list[dict] | None, state: UserState) -> Generator[tuple[list[dict], str], None, None]:
        if not message:
            yield history, ""
            return

        if history is None:
            history = []

        # Add user message to history immediately
        history.append({"role": "user", "content": message})
        yield history, ""

        # Add loading message in chatbot
        history.append({"role": "assistant", "content": "â³ æ­£åœ¨æ¨ç†..."})
        yield history, ""

        # Stream the response
        response_text = ""
        for chunk in chat_response(message, history[:-2], state):
            response_text += chunk
            yield history[:-1] + [{"role": "assistant", "content": response_text}], ""

    submit.click(
        submit_message,
        inputs=[msg, chatbot, user_state],
        outputs=[chatbot, msg]
    )

    clear.click(
        lambda: None,
        outputs=[chatbot]
    )

    # Model selection handler
    def on_model_change(model_id: str, state: UserState) -> tuple[UserState, str, dict]:
        state = update_model(model_id, state)
        model_config = get_model_config(model_id)

        # Get URL for the selected model
        url = model_config.get('base_url', '')

        # Update thinking checkbox based on model support
        supports_thinking = model_config.get('supports_thinking', False)
        if not supports_thinking:
            state["enable_thinking"] = False

        # å¦‚æœæ¨¡å‹æ”¯æŒæ€è€ƒï¼Œä¿æŒç”¨æˆ·å½“å‰çš„é€‰æ‹©ï¼›ä¸æ”¯æŒåˆ™å¼ºåˆ¶è®¾ä¸º False
        checkbox_value = state["enable_thinking"] if supports_thinking else False
        # è¿”å›æ›´æ–°åçš„çŠ¶æ€ã€URLã€ä»¥åŠ checkbox çš„æ›´æ–°é…ç½®ï¼ˆå€¼å’Œæ˜¯å¦å¯äº¤äº’ï¼‰
        return state, url, gr.update(value=checkbox_value, interactive=supports_thinking)

    model_dropdown.change(
        on_model_change,
        inputs=[model_dropdown, user_state],
        outputs=[user_state, model_url_display, show_thinking]
    )

    update_context_btn.click(
        update_context_size,
        inputs=[context_size, user_state],
        outputs=[user_state]
    )

    update_prompt_btn.click(
        update_system_prompt,
        inputs=[system_prompt, user_state],
        outputs=[user_state]
    )

    def toggle_thinking_enable(show: bool, state: UserState) -> UserState:
        state["enable_thinking"] = show
        return state

    show_thinking.change(
        toggle_thinking_enable,
        inputs=[show_thinking, user_state],
        outputs=[user_state]
    )


if __name__ == "__main__":
    logger.warning("æœåŠ¡å™¨æ­£åœ¨å¯åŠ¨...")
    demo.queue(
        default_concurrency_limit=5,   # æ”¯æŒ5ä¸ªå¹¶å‘å¯¹è¯
        max_size=100                   # é˜Ÿåˆ—æœ€å¤§é•¿åº¦
    ).launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
    logger.warning("æœåŠ¡å™¨å·²å…³é—­")
